{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some information about environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n",
      "Box(2,)\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for episod in range(100):\n",
    "    state = env.reset()\n",
    "    episod_reward = 0\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episod_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            total_rewards.append(episod_reward)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-200.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task was solved 0 times with random action algorithm\n"
     ]
    }
   ],
   "source": [
    "total_rewards = np.array(total_rewards)\n",
    "print(f\"Task was solved {len(total_rewards[total_rewards > -200])} times with random action algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that provided task can't be solved with simple policy of taking random actions. Let's try to learn optimal policy with deep Q learning algorithm. More precisely, we will learn Q values and than take based on them choose next action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space_dim = env.observation_space.shape[0]\n",
    "action_space_dim = env.action_space.n\n",
    "hidden_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_dim=hidden_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(observation_space_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, action_space_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, the target labels are not changing over training. That makes training stable. Since in Q learning estimation of Q-values is based on another Q-values we are dealing with continiously changing target. To make RL model more stable we will use two models. One will be fixed till another one is training. Once in a while we will update first model too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we will use Experience replay technique. Good motivation and explonation of it is provided in [Dealing with Sparse Rewards in Reinforcement Learning](https://arxiv.org/abs/1910.09281):\n",
    "\n",
    "If using a gradient based iterative optimisation process as done via back-propagation in\n",
    "neural networks, it is important that the data is independent and identically distributed (i.i.d.).\n",
    "This is done as to avoid sampling bias from correlated inputs, which can cause the gradient to get\n",
    "stuck in a non-optimal local maxima (as we are performing gradient ascent to maximise the expected\n",
    "future reward). Experience replay is a method to help decorrelate the sequential experiences gained\n",
    "from dynamic programming and model free reinforcement learning methods. This is done by storing\n",
    "experiences, a tuple of (st, at, rt, st+1) into a list of experiences known as the replay memory. Batch\n",
    "samples can be drawn randomly from the replay memory which provide âˆ¼ i.i.d. for a large replay\n",
    "length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience:\n",
    "    \"\"\"Class for implementing Experience Replay technique\"\"\"\n",
    "    def __init__(self, capacity, weighted=False):\n",
    "        self.capacity = capacity\n",
    "        self.experience = []\n",
    "        self.weighted = weighted\n",
    "\n",
    "    def add(self, el):\n",
    "        self.experince.append(el)\n",
    "        self.experince = self.experince[-size:]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.weighted:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            return np.random.choice(self.experience, size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one possible improvement in base Experience Replay method regarding the way of sampling. We can use weighted sampling to put more attention to states with greater reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit network we have to calculate predicted by model Q-values and target Q-values created by fixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, fixed_model, data, optimizer, criterion):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major difficulty of Mountain car task is extremely sparce rewards: reward is -1 after each step till goal is not reached. To make reward function more dense we will add some heuristic to it.\n",
    "\n",
    "Let's first define base reward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_base_reward(state, reward, next_state):\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we will use eps-greedy policy with decreasing eps over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(model, state, eps=0):\n",
    "    with torch.no_grad():\n",
    "        if np.random.rand() <= eps:\n",
    "            return np.random.choice(action_space_dim)\n",
    "        else:\n",
    "            output = model(torch.tensor(state).float())\n",
    "            return torch.argmax(output).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, draw=False):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if draw:\n",
    "            env.render()\n",
    "        action = greedy_action(model, state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    state = env.reset()  \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, criterion, reward_function, \n",
    "          time_max=100000, batchsize=64, fixed_model_update=100,\n",
    "          eps_min=0.1, eps_max=0.9):\n",
    "    total_rewards = []\n",
    "    \n",
    "    model = create_model()\n",
    "    fixed_model = copy.deepcopy(model)\n",
    "    \n",
    "    experience = Experience()\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for time in range(max_time):\n",
    "        eps = eps_max - (eps_max - eps_min) * time / time_max\n",
    "        action = greedy_action(model, state, eps)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        experience.add([\n",
    "            state,\n",
    "            action,\n",
    "            reward_function(state, reward, next_state),\n",
    "            next_state,\n",
    "            done\n",
    "        ])\n",
    "        \n",
    "        if done: # Restart simulation\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "        if time > batchsize:\n",
    "            fit(model, fixed_model, experience.sample(batchsize, optimizer, criterion) # Train model\n",
    "                \n",
    "        if time % fixed_model_update == 0:\n",
    "            fixed_model = copy.deepcopy(model)\n",
    "            total_rewards.append(validate_model(fixed_model))\n",
    "    return model, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00003)\n",
    "criterion = F.mse_loss\n",
    "time_max = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, total_rewards = train(optimizer,\n",
    "                             criterion,\n",
    "                             calc_base_reward,\n",
    "                             time_max=time_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(total_rewards)), total_rewards)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = validate_model(model, draw=True)\n",
    "print(f\"Reward: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
